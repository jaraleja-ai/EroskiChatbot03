# =====================================================
# database/scripts/process_manual_text.py - Procesar manual sin vectores
# =====================================================
"""
Script para procesar el manual de la balanza DIBAL Mistral
usando b√∫squeda de texto (sin vectores).
"""

import asyncio
import asyncpg
import logging
import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import re

# Setup path
ROOT_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(ROOT_DIR))

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("ProcessManual")

class ManualTextProcessor:
    """Procesador de manual t√©cnico usando b√∫squeda de texto"""
    
    def __init__(self):
        # Configuraci√≥n de BD
        self.db_config = {
            "host": os.getenv("DB_HOST", "localhost"),
            "port": int(os.getenv("DB_PORT", 5432)),
            "database": os.getenv("DB_NAME", "chatbot_db"),
            "user": os.getenv("DB_USER", "postgres"),
            "password": os.getenv("DB_PASSWORD", "")
        }
        
    @property
    def connection_string(self) -> str:
        """Construir string de conexi√≥n"""
        return (f"postgresql://{self.db_config['user']}:{self.db_config['password']}"
                f"@{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
    
    async def process_manual(self):
        """Proceso completo de procesamiento del manual"""
        try:
            logger.info("üìñ Iniciando procesamiento del manual DIBAL Mistral...")
            
            # 1. Verificar dependencias
            if not self._check_dependencies():
                return False
            
            # 2. Verificar archivo existe
            manual_path = ROOT_DIR / "docs" / "Manual Balanza DIBAL Mistral.pdf"
            if not manual_path.exists():
                logger.error(f"‚ùå Manual no encontrado: {manual_path}")
                logger.info("üí° Aseg√∫rate de que el archivo est√© en docs/")
                return False
            
            # 3. Verificar si ya est√° procesado
            if await self._is_already_processed(manual_path.name):
                logger.info("‚úÖ Manual ya procesado")
                response = input("¬øDeseas re-procesar? (s/n): ").lower().strip()
                if response not in ['s', 'si', 's√≠', 'y', 'yes']:
                    return True
                await self._clear_existing_chunks(manual_path.name)
            
            # 4. Procesar PDF
            chunks = await self._process_pdf(manual_path)
            if not chunks:
                return False
            
            # 5. Almacenar chunks
            success = await self._store_chunks(chunks, manual_path.name)
            
            if success:
                logger.info("üéâ Manual procesado exitosamente")
                await self._show_summary()
                return True
            else:
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Error procesando manual: {e}")
            return False
    
    def _check_dependencies(self):
        """Verificar que las dependencias est√°n instaladas"""
        try:
            import langchain.document_loaders
            import langchain.text_splitter
            
            logger.info("‚úÖ Dependencias verificadas")
            return True
            
        except ImportError as e:
            logger.error(f"‚ùå Dependencia faltante: {e}")
            logger.info("üí° Instalar: pip install langchain pypdf2")
            return False
    
    async def _is_already_processed(self, documento_nombre: str) -> bool:
        """Verificar si el documento ya est√° procesado"""
        try:
            conn = await asyncpg.connect(self.connection_string)
            count = await conn.fetchval(
                "SELECT COUNT(*) FROM knowledge_base WHERE documento_origen = $1",
                documento_nombre
            )
            await conn.close()
            return count > 0
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error verificando procesamiento: {e}")
            return False
    
    async def _clear_existing_chunks(self, documento_nombre: str):
        """Limpiar chunks existentes"""
        try:
            conn = await asyncpg.connect(self.connection_string)
            deleted = await conn.fetchval(
                "DELETE FROM knowledge_base WHERE documento_origen = $1 RETURNING COUNT(*)",
                documento_nombre
            )
            await conn.close()
            logger.info(f"üóëÔ∏è {deleted} chunks existentes eliminados")
        except Exception as e:
            logger.error(f"‚ùå Error limpiando chunks: {e}")
    
    async def _process_pdf(self, pdf_path: Path) -> List[Dict[str, Any]]:
        """Procesar PDF y crear chunks"""
        try:
            from langchain.document_loaders import PyPDFLoader
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            
            logger.info(f"üìÑ Cargando PDF: {pdf_path.name}")
            
            # Cargar PDF
            loader = PyPDFLoader(str(pdf_path))
            documents = loader.load()
            
            logger.info(f"üìã P√°ginas cargadas: {len(documents)}")
            
            # Configurar text splitter optimizado para manuales t√©cnicos
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,  # Tama√±o √≥ptimo para b√∫squeda
                chunk_overlap=200,  # Overlap para preservar contexto
                separators=[
                    "\n\n",  # P√°rrafos
                    "\n",    # L√≠neas
                    ". ",    # Oraciones
                    " "      # Palabras
                ],
                length_function=len,
                keep_separator=True
            )
            
            # Crear chunks
            chunks = text_splitter.split_documents(documents)
            
            # Filtrar chunks muy cortos
            filtered_chunks = []
            for chunk in chunks:
                content = chunk.page_content.strip()
                if len(content) >= 50 and not content.isspace():
                    filtered_chunks.append({
                        'content': content,
                        'metadata': chunk.metadata,
                        'page': chunk.metadata.get('page', 0),
                        'source': chunk.metadata.get('source', str(pdf_path))
                    })
            
            logger.info(f"üìù Chunks creados: {len(filtered_chunks)} (filtrados de {len(chunks)})")
            return filtered_chunks
            
        except Exception as e:
            logger.error(f"‚ùå Error procesando PDF: {e}")
            return []
    
    async def _store_chunks(self, chunks: List[Dict[str, Any]], documento_nombre: str) -> bool:
        """Almacenar chunks en BD (sin vectores)"""
        try:
            import json
            conn = await asyncpg.connect(self.connection_string)
            
            logger.info(f"üîÑ Almacenando {len(chunks)} chunks...")
            
            try:
                total_chunks = len(chunks)
                
                for i, chunk in enumerate(chunks, 1):
                    # Progress indicator
                    if i % 10 == 0 or i == total_chunks:
                        logger.info(f"üîÑ Procesando chunk {i}/{total_chunks} ({i/total_chunks*100:.1f}%)")
                    
                    # Extraer metadata adicional
                    metadata = self._extract_metadata(chunk['content'], chunk['metadata'])
                    
                    # Insertar en BD (sin embedding)
                    await conn.execute("""
                        INSERT INTO knowledge_base 
                        (documento_origen, chunk_text, pagina_numero, seccion, capitulo, 
                         palabras_clave, chunk_metadata, embedding_pendiente)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                    """, 
                        documento_nombre,
                        chunk['content'],
                        chunk['page'],
                        metadata['seccion'],
                        metadata['capitulo'],
                        metadata['palabras_clave'],
                        json.dumps(metadata),  # Convertir a JSON
                        True  # embedding_pendiente = True
                    )
                
                logger.info("‚úÖ Todos los chunks almacenados")
                return True
                
            finally:
                await conn.close()
                
        except Exception as e:
            logger.error(f"‚ùå Error almacenando chunks: {e}")
            return False
    
    def _extract_metadata(self, text: str, original_metadata: Dict) -> Dict[str, Any]:
        """Extraer metadata adicional del texto"""
        metadata = {
            'seccion': self._extract_section(text),
            'capitulo': self._extract_chapter(text),
            'palabras_clave': self._extract_keywords(text),
            'longitud': len(text),
            'es_tabla': self._is_table_content(text),
            'es_lista': self._is_list_content(text),
            'contiene_codigo': self._contains_error_code(text),
            'tipo_contenido': self._classify_content_type(text)
        }
        
        # Agregar metadata original
        metadata.update(original_metadata)
        
        return metadata
    
    def _extract_section(self, text: str) -> str:
        """Extraer nombre de secci√≥n del texto"""
        lines = text.split('\n')
        
        for line in lines[:3]:  # Revisar primeras 3 l√≠neas
            line = line.strip()
            
            # Patrones comunes en manuales t√©cnicos
            section_patterns = [
                r'^(\d+\.?\d*\.?\d*)\s+(.+)',  # "1.2.3 T√≠tulo"
                r'^([A-Z][A-Z√Å√â√ç√ì√ö√ë√ú\s]+)$',   # "CAP√çTULO MAY√öSCULAS"
                r'.*[Cc]ap√≠tulo\s+(\d+)',      # "Cap√≠tulo X"
                r'.*[Ss]ecci√≥n\s+(\d+)',       # "Secci√≥n X"
                r'.*[Pp]arte\s+(\d+)',         # "Parte X"
            ]
            
            for pattern in section_patterns:
                match = re.match(pattern, line)
                if match and len(line) < 100:
                    return line[:200]
        
        return "General"
    
    def _extract_chapter(self, text: str) -> str:
        """Extraer cap√≠tulo del texto"""
        chapter_patterns = [
            r'[Cc]ap√≠tulo\s+(\d+)[:\-\s]*(.{0,50})',
            r'[Cc]hapter\s+(\d+)[:\-\s]*(.{0,50})',
            r'^(\d+)\.\s*([A-Z√Å√â√ç√ì√ö√ë√ú][^.]{5,50})',
        ]
        
        for pattern in chapter_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)[:200]
        
        return ""
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extraer palabras clave relevantes"""
        keywords = []
        
        # T√©rminos espec√≠ficos de balanzas DIBAL
        balanza_terms = [
            'balanza', 'peso', 'tara', 'calibrar', 'calibraci√≥n',
            'etiqueta', 'precio', 'imprimir', 'display', 'teclado',
            'dibal', 'mistral', 'b√°scula', 'pesaje', 'error',
            'configuraci√≥n', 'men√∫', 'funci√≥n', 'bot√≥n', 'pantalla',
            'memoria', 'producto', 'c√≥digo', 'PLU', 'ticket',
            'impresora', 'conexi√≥n', 'cable', 'instalaci√≥n'
        ]
        
        text_lower = text.lower()
        
        for term in balanza_terms:
            if term in text_lower:
                keywords.append(term)
        
        # Buscar c√≥digos de error
        error_codes = re.findall(r'[Ee]rror?\s*:?\s*(\d+)', text)
        for code in error_codes:
            keywords.append(f"error_{code}")
        
        # Buscar n√∫meros de modelo
        models = re.findall(r'(DIBAL|dibal)\s+(\w+)', text, re.IGNORECASE)
        for model in models:
            keywords.append(f"modelo_{model[1].lower()}")
        
        return keywords[:10]  # M√°ximo 10 keywords
    
    def _is_table_content(self, text: str) -> bool:
        """Detectar si el contenido es una tabla"""
        lines = text.split('\n')
        tab_count = sum(1 for line in lines if '\t' in line or '|' in line)
        return tab_count > len(lines) * 0.3
    
    def _is_list_content(self, text: str) -> bool:
        """Detectar si el contenido es una lista"""
        lines = text.split('\n')
        list_indicators = ['-', '‚Ä¢', '*', '1.', '2.', 'a)', 'b)']
        list_count = sum(1 for line in lines 
                        if any(line.strip().startswith(indicator) 
                              for indicator in list_indicators))
        return list_count > len(lines) * 0.5
    
    def _contains_error_code(self, text: str) -> bool:
        """Detectar si contiene c√≥digos de error"""
        error_patterns = [
            r'[Ee]rror\s*:?\s*\d+',
            r'[Cc]√≥digo\s*:?\s*\d+',
            r'[Ff]allo\s*:?\s*\d+'
        ]
        
        return any(re.search(pattern, text) for pattern in error_patterns)
    
    def _classify_content_type(self, text: str) -> str:
        """Clasificar tipo de contenido"""
        if self._is_table_content(text):
            return "tabla"
        elif self._is_list_content(text):
            return "lista"
        elif self._contains_error_code(text):
            return "error_code"
        elif any(term in text.lower() for term in ['instalaci√≥n', 'conexi√≥n', 'configurar']):
            return "instalacion"
        elif any(term in text.lower() for term in ['calibrar', 'calibraci√≥n', 'ajuste']):
            return "calibracion"
        elif any(term in text.lower() for term in ['error', 'problema', 'fallo']):
            return "solucion_problemas"
        else:
            return "general"
    
    async def _show_summary(self):
        """Mostrar resumen del procesamiento"""
        try:
            conn = await asyncpg.connect(self.connection_string)
            
            # Estad√≠sticas generales
            total_chunks = await conn.fetchval(
                "SELECT COUNT(*) FROM knowledge_base"
            )
            
            total_docs = await conn.fetchval(
                "SELECT COUNT(DISTINCT documento_origen) FROM knowledge_base"
            )
            
            # Estad√≠sticas por secci√≥n
            sections = await conn.fetch("""
                SELECT seccion, COUNT(*) as chunks
                FROM knowledge_base 
                WHERE seccion != 'General'
                GROUP BY seccion 
                ORDER BY chunks DESC 
                LIMIT 5
            """)
            
            # Estad√≠sticas por tipo de contenido
            content_types = await conn.fetch("""
                SELECT 
                    chunk_metadata->>'tipo_contenido' as tipo,
                    COUNT(*) as chunks
                FROM knowledge_base 
                WHERE chunk_metadata->>'tipo_contenido' IS NOT NULL
                GROUP BY chunk_metadata->>'tipo_contenido'
                ORDER BY chunks DESC
            """)
            
            await conn.close()
            
            logger.info("üìä RESUMEN DEL PROCESAMIENTO:")
            logger.info(f"   üìö Documentos procesados: {total_docs}")
            logger.info(f"   üìÑ Total chunks: {total_chunks}")
            
            if sections:
                logger.info("   üîç Top secciones:")
                for section in sections:
                    logger.info(f"      ‚Ä¢ {section['seccion']}: {section['chunks']} chunks")
            
            if content_types:
                logger.info("   üìã Tipos de contenido:")
                for ctype in content_types:
                    logger.info(f"      ‚Ä¢ {ctype['tipo']}: {ctype['chunks']} chunks")
            
            logger.info("\nüéØ PR√ìXIMOS PASOS:")
            logger.info("   1. üß™ Probar b√∫squeda: python database/scripts/test_text_search.py")
            logger.info("   2. üöÄ Ejecutar chatbot: python main.py")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error mostrando resumen: {e}")

async def main():
    """Entry point del script"""
    # Cargar variables de entorno
    current_dir = Path(__file__).parent
    root_dir = current_dir.parent.parent
    env_file = root_dir / ".env"
    
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                if '=' in line and not line.strip().startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value.strip('"\'')
    
    print("üìñ Procesador de Manual DIBAL Mistral")
    print("=" * 45)
    print("üîç M√âTODO: B√∫squeda de texto")
    print("üìö SIN VECTORES: Funcional y eficiente")
    print("=" * 45)
    
    processor = ManualTextProcessor()
    success = await processor.process_manual()
    
    if success:
        print("\n‚úÖ ¬°Manual procesado exitosamente!")
    else:
        print("\n‚ùå Error en el procesamiento")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())