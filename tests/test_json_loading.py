# =====================================================
# nodes/classify_llm_driven.py - Nodo de Clasificaci√≥n LLM-Driven
# =====================================================
"""
Nodo de clasificaci√≥n inteligente dirigido completamente por LLM.

FUNCIONALIDAD:
1. LLM analiza el historial de mensajes para identificar incidencias
2. Identifica tipo de incidencia usando keywords del archivo JSON
3. Hace preguntas espec√≠ficas para detectar el problema exacto
4. Propone soluci√≥n cuando identifica el problema
5. Escala a supervisor si no puede identificar la incidencia
6. Mantiene conversaci√≥n natural durante todo el proceso
"""

from typing import Dict, Any, Optional, List, Union
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.types import Command
from datetime import datetime
import logging
import json
import re
from pathlib import Path

from models.eroski_state import EroskiState
from nodes.base_node import BaseNode
from utils.llm.providers import get_llm
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from config.incident_config import IncidentConfigLoader


# =============================================================================
# MODELOS DE DATOS
# =============================================================================

class ClassificationDecision(BaseModel):
    """Decisi√≥n del LLM sobre la clasificaci√≥n de incidencia"""
    
    # Estado de la clasificaci√≥n
    incident_identified: bool = Field(description="Si se ha identificado el tipo de incidencia")
    problem_identified: bool = Field(description="Si se ha identificado el problema espec√≠fico")
    solution_ready: bool = Field(description="Si se puede proporcionar una soluci√≥n")
    needs_escalation: bool = Field(description="Si debe escalarse a supervisor")
    wants_to_cancel: bool = Field(description="Si el usuario quiere cancelar")
    
    # Informaci√≥n identificada
    incident_type: Optional[str] = Field(description="Tipo de incidencia identificada", default=None)
    specific_problem: Optional[str] = Field(description="Problema espec√≠fico identificado", default=None)
    proposed_solution: Optional[str] = Field(description="Soluci√≥n propuesta", default=None)
    confidence_level: float = Field(description="Confianza en la identificaci√≥n (0-1)", default=0.0)
    
    # Pr√≥xima acci√≥n
    next_action: str = Field(description="Pr√≥xima acci√≥n: ask_questions, provide_solution, escalate, complete")
    message_to_user: str = Field(description="Mensaje natural para el usuario")
    questions_to_ask: List[str] = Field(description="Preguntas espec√≠ficas si necesita m√°s informaci√≥n", default=[])
    
    # Informaci√≥n extra√≠da del √∫ltimo mensaje
    keywords_detected: List[str] = Field(description="Keywords detectadas en el mensaje", default=[])
    urgency_level: int = Field(description="Nivel de urgencia detectado (1-4)", default=2)
    
    # ‚úÖ NUEVO: Informaci√≥n hist√≥rica
    historical_info_found: Optional[str] = Field(description="Informaci√≥n relevante encontrada en el historial", default=None)


# =============================================================================
# NODO PRINCIPAL CLASSIFY LLM-DRIVEN
# =============================================================================

class LLMDrivenClassifyNode(BaseNode):
    """
    Nodo de clasificaci√≥n completamente dirigido por LLM.
    
    CARACTER√çSTICAS:
    - An√°lisis inteligente del historial de mensajes
    - Identificaci√≥n autom√°tica usando keywords del JSON
    - Conversaci√≥n natural para recopilar informaci√≥n faltante
    - Propuesta de soluciones autom√°ticas
    - Escalaci√≥n inteligente cuando no puede resolver
    """
    
    def __init__(self):
        super().__init__("classify")
        self.max_attempts = 8  # M√°s intentos para conversaci√≥n detallada
        self.llm = get_llm()
        
        # Cargar configuraci√≥n de incidencias
        self.incident_loader = IncidentConfigLoader()
        self.incident_types = self._load_incident_types()
        
        # Parser para decisiones LLM
        self.parser = JsonOutputParser(pydantic_object=ClassificationDecision)
        
        # Sistema principal de conversaci√≥n
        self.classification_prompt = self._build_classification_prompt()
        
        # ‚úÖ NUEVO: Prompt especializado para an√°lisis hist√≥rico
        self.historical_analysis_prompt = self._build_historical_analysis_prompt()
        
    def get_required_fields(self) -> List[str]:
        return ["messages", "authenticated"]
    
    def get_actor_description(self) -> str:
        return "Clasifico incidencias t√©cnicas usando IA conversacional y propongo soluciones"
    
    def _load_incident_types(self) -> Dict[str, Any]:
        """Cargar tipos de incidencia desde el archivo JSON"""
        try:
            # Usar directamente el archivo JSON sin IncidentConfigLoader por ahora
            json_path = Path("config/eroski_incidents.json")
            if not json_path.exists():
                json_path = Path("scripts/eroski_incidents.json")
            
            if json_path.exists():
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # Verificar si usa la estructura "tipo_incidente" o "incident_types"
                    if "tipo_incidente" in data:
                        return data["tipo_incidente"]
                    elif "incident_types" in data:
                        return data["incident_types"]
                    else:
                        self.logger.warning("‚ùå Estructura de JSON no reconocida")
                        return {}
                        
            self.logger.warning("‚ùå No se pudo cargar archivo de incidencias")
            return {}
            
        except Exception as e:
            self.logger.error(f"‚ùå Error cargando tipos de incidencia: {e}")
            return {}
    
    def _build_classification_prompt(self) -> PromptTemplate:
        """Construir prompt principal para clasificaci√≥n LLM-driven"""
        return PromptTemplate(
            template="""Eres un asistente experto en clasificaci√≥n de incidencias t√©cnicas para Eroski.

Tu misi√≥n es analizar los mensajes del usuario para:
1. IDENTIFICAR si describe una incidencia t√©cnica
2. CLASIFICAR el tipo de incidencia usando las categor√≠as disponibles
3. DETECTAR el problema espec√≠fico dentro de ese tipo
4. PROPONER una soluci√≥n si la conoces
5. HACER PREGUNTAS inteligentes si necesitas m√°s informaci√≥n
6. ESCALAR a supervisor si no puedes identificar la incidencia

TIPOS DE INCIDENCIAS DISPONIBLES:
{incident_types_info}

HISTORIAL COMPLETO DE MENSAJES DEL USUARIO:
{conversation_history}

DATOS DEL EMPLEADO AUTENTICADO:
- Nombre: {employee_name}
- Tienda: {store_name}
- Secci√≥n: {section}

ESTADO ACTUAL DE CLASIFICACI√ìN:
- Intentos realizados: {attempt_number}
- Tipo identificado anteriormente: {previous_incident_type}
- Problema identificado: {previous_problem}

√öLTIMO MENSAJE DEL USUARIO:
"{user_message}"

INSTRUCCIONES PARA TU AN√ÅLISIS:

1. **ANALIZAR TODO EL HISTORIAL**: No solo el √∫ltimo mensaje, sino toda la conversaci√≥n
2. **IDENTIFICAR KEYWORDS**: Busca palabras clave que coincidan con los tipos de incidencia
3. **EVALUAR CONTEXTO**: Considera la secci√≥n del empleado (ej: si est√° en "Caja" y habla de problemas, probablemente sea TPV)
4. **SER ESPEC√çFICO**: No te conformes con identificar solo el tipo, busca el problema exacto
5. **PROPONER SOLUCIONES**: Si identificas el problema espec√≠fico, proporciona la soluci√≥n correspondiente
6. **HACER PREGUNTAS INTELIGENTES**: Si falta informaci√≥n, haz preguntas espec√≠ficas y √∫tiles
7. **ESCALAR CUANDO SEA NECESARIO**: Si despu√©s de varios intentos no puedes clasificar, escala

CRITERIOS DE ESCALACI√ìN:
- Han pasado m√°s de 5 intentos sin identificar claramente la incidencia
- El usuario describe algo que no est√° en ninguna categor√≠a
- El problema parece muy t√©cnico o complejo
- El usuario est√° frustrado o pide hablar con un supervisor

RESPONDE √öNICAMENTE CON JSON V√ÅLIDO incluyendo TODOS los campos:
{{
    "incident_identified": false,
    "problem_identified": false,
    "solution_ready": false,
    "needs_escalation": false,
    "wants_to_cancel": false,
    "incident_type": null,
    "specific_problem": null,
    "proposed_solution": null,
    "confidence_level": 0.0,
    "next_action": "ask_questions",
    "message_to_user": "Mensaje natural aqu√≠",
    "questions_to_ask": [],
    "keywords_detected": [],
    "urgency_level": 2
}}

‚ö†Ô∏è IMPORTANTE: No uses formato Markdown. Solo responde con JSON puro, sin s√≠mbolos adicionales.
""",
            input_variables=[
                "incident_types_info", "conversation_history", "employee_name", 
                "store_name", "section"
            ]
        )
    
    def _build_historical_analysis_prompt(self) -> PromptTemplate:
        """Construir prompt especializado para an√°lisis hist√≥rico inicial"""
        return PromptTemplate(
            template="""Eres un asistente experto en clasificaci√≥n de incidencias t√©cnicas para Eroski.

Esta es tu PRIMERA INTERACCI√ìN con el usuario en el nodo de clasificaci√≥n. Tu tarea es analizar TODO EL HISTORIAL de mensajes para:

1. **IDENTIFICAR** si el usuario ya mencion√≥ informaci√≥n sobre una incidencia t√©cnica
2. **EXTRAER** toda la informaci√≥n relevante que ya proporcion√≥
3. **CLASIFICAR** la incidencia si tienes suficiente informaci√≥n
4. **CONTINUAR** la conversaci√≥n de forma natural sin repetir preguntas

TIPOS DE INCIDENCIAS DISPONIBLES:
{incident_types_info}

REGLAS ESTRICTAS:
‚ùå NO inventes soluciones que no est√©n en el archivo de configuraci√≥n
‚ùå NO des consejos gen√©ricos si no tienes la soluci√≥n espec√≠fica
‚úÖ SOLO usa las soluciones que aparecen en el archivo para cada problema
‚úÖ Si no encuentras el problema espec√≠fico en el archivo, pregunta m√°s detalles
‚úÖ Si despu√©s de preguntar no puedes clasificar, ESCALA al supervisor

DATOS DEL EMPLEADO AUTENTICADO:
- Nombre: {employee_name}
- Tienda: {store_name}
- Secci√≥n: {section}

HISTORIAL COMPLETO DE MENSAJES (ANALIZA TODO):
{conversation_history}

INSTRUCCIONES PARA TU AN√ÅLISIS:

üîç **AN√ÅLISIS HIST√ìRICO OBLIGATORIO:**
- Revisa TODOS los mensajes del usuario desde el principio
- Busca cualquier menci√≥n de problemas, equipos, errores, fallos
- Identifica keywords que coincidan EXACTAMENTE con los tipos de incidencia
- Considera el contexto de la secci√≥n del empleado

üéØ **ESTRATEGIAS DE IDENTIFICACI√ìN:**
1. **Keywords directas**: "balanza", "TPV", "impresora", "red", etc.
2. **Problemas descritos**: "no funciona", "error", "no imprime", "no enciende"
3. **Contexto seccional**: Si est√° en "carnicer√≠a" y menciona problemas, probablemente balanza
4. **Urgencia impl√≠cita**: Palabras como "urgente", "cr√≠tico", "parado"

üß† **L√ìGICA DE DECISI√ìN ESTRICTA:**
- Si identificas el tipo Y el problema EST√Å en el archivo ‚Üí proponer la soluci√≥n DEL ARCHIVO
- Si identificas el tipo pero el problema NO EST√Å en el archivo ‚Üí hacer preguntas ESPEC√çFICAS basadas en los problemas disponibles en el archivo
- Si NO identificas el tipo ‚Üí hacer preguntas abiertas para clasificar
- Si es confuso o no est√° catalogado ‚Üí preparar para escalaci√≥n

‚úÖ **PREGUNTAS ESPEC√çFICAS OBLIGATORIAS:**
Cuando identifiques el tipo pero no el problema espec√≠fico, debes:
1. Listar los problemas espec√≠ficos disponibles en el archivo para ese tipo
2. Preguntar directamente cu√°l de esos problemas coincide
3. No hacer preguntas gen√©ricas como "cu√©ntame m√°s detalles"

EJEMPLO CORRECTO:
Usuario menciona "balanza no funciona" ‚Üí 
Respuesta: "Entiendo que hay un problema con la balanza. Seg√∫n nuestro cat√°logo, los problemas m√°s comunes con balanzas son:
‚Ä¢ La balanza no imprime las etiquetas
‚Ä¢ La balanza no se enciende  
‚Ä¢ El precio que imprime en la etiqueta no es correcto
‚Ä¢ La pantalla no responde
‚Ä¢ Error de calibraci√≥n
‚Ä¢ Etiquetas salen en blanco

¬øCu√°l de estos describe mejor tu problema?"

‚úÖ **EJEMPLO DE AN√ÅLISIS CORRECTO:**
Usuario menciona "balanza no imprime etiquetas" ‚Üí 
1. Identifica tipo: "balanza"
2. Busca problema "no imprime etiquetas" en el archivo balanza
3. Si est√°: propone la soluci√≥n exacta del archivo
4. Si no est√°: pregunta m√°s detalles espec√≠ficos sobre balanzas

RESPONDE √öNICAMENTE CON JSON V√ÅLIDO incluyendo TODOS los campos:
{{
    "incident_identified": false,
    "problem_identified": false,
    "solution_ready": false,
    "needs_escalation": false,
    "wants_to_cancel": false,
    "incident_type": null,
    "specific_problem": null,
    "proposed_solution": null,
    "confidence_level": 0.0,
    "next_action": "ask_questions",
    "message_to_user": "Mensaje natural aqu√≠",
    "questions_to_ask": [],
    "keywords_detected": [],
    "urgency_level": 2,
    "historical_info_found": "Descripci√≥n de la informaci√≥n encontrada en el historial"
}}

‚ö†Ô∏è IMPORTANTE: No uses formato Markdown. Solo responde con JSON puro, sin s√≠mbolos adicionales.
""",
            input_variables=[
                "incident_types_info", "conversation_history", "employee_name", 
                "store_name", "section", "attempt_number", "previous_incident_type",
                "previous_problem", "user_message"
            ]
        )
    
    def _format_incident_types_for_llm(self) -> str:
        """Formatear tipos de incidencia para el LLM"""
        if not self.incident_types:
            return "No hay tipos de incidencia configurados"
        
        formatted = []
        for incident_id, incident_data in self.incident_types.items():
            name = incident_data.get("name", incident_id)
            description = incident_data.get("description", "")
            keywords = incident_data.get("keywords", [])
            
            # Verificar si tiene la estructura "problemas" (nueva) o "common_issues" (antigua)
            problems = []
            if "problemas" in incident_data:
                # Nueva estructura con problemas y soluciones
                problems = list(incident_data["problemas"].keys())[:3]
            elif "common_issues" in incident_data:
                # Estructura antigua
                problems = incident_data["common_issues"][:3]
            
            urgency = incident_data.get("urgency_level", 2)
            
            formatted.append(f"""
**{incident_id.upper()}** - {name}
- Descripci√≥n: {description}
- Keywords: {', '.join(keywords)}
- Urgencia: {urgency}/4
- Problemas comunes: {', '.join(problems)}{"..." if len(problems) > 3 else ""}
""")
        
        return "\n".join(formatted)
    
    def _get_specific_solution(self, incident_type: str, problem_description: str) -> Optional[str]:
        """
        Buscar soluci√≥n espec√≠fica en el archivo JSON para un problema dado.
        
        Args:
            incident_type: Tipo de incidencia (ej: "balanza")
            problem_description: Descripci√≥n del problema
            
        Returns:
            Soluci√≥n espec√≠fica o None si no se encuentra
        """
        if incident_type not in self.incident_types:
            return None
        
        incident_data = self.incident_types[incident_type]
        
        # Verificar si tiene la estructura "problemas" (nueva)
        if "problemas" in incident_data:
            problems = incident_data["problemas"]
            
            # Buscar coincidencia exacta o parcial
            problem_lower = problem_description.lower()
            
            for problem_key, solution in problems.items():
                problem_key_lower = problem_key.lower()
                
                # Coincidencia exacta
                if problem_lower == problem_key_lower:
                    return solution
                
                # Coincidencia parcial (palabras clave)
                if any(word in problem_key_lower for word in problem_lower.split() if len(word) > 3):
                    return solution
        
        return None
    
    def _find_best_matching_problem(self, incident_type: str, user_description: str) -> tuple[Optional[str], Optional[str]]:
        """
        Encontrar el problema que mejor coincida con la descripci√≥n del usuario.
        
        Args:
            incident_type: Tipo de incidencia
            user_description: Descripci√≥n del problema por el usuario
            
        Returns:
            Tupla (problema_identificado, soluci√≥n) o (None, None)
        """
        if incident_type not in self.incident_types:
            return None, None
        
        incident_data = self.incident_types[incident_type]
        
        if "problemas" not in incident_data:
            return None, None
        
        problems = incident_data["problemas"]
        user_desc_lower = user_description.lower()
        
        best_match = None
        best_solution = None
        best_score = 0
        
        for problem_key, solution in problems.items():
            problem_lower = problem_key.lower()
            
            # Calcular score de coincidencia
            score = 0
            user_words = set(user_desc_lower.split())
            problem_words = set(problem_lower.split())
            
            # Palabras en com√∫n
            common_words = user_words.intersection(problem_words)
            if common_words:
                score += len(common_words) * 2
            
            # Substrings en com√∫n
            if any(word in problem_lower for word in user_words if len(word) > 3):
                score += 1
            
            if score > best_score:
                best_score = score
                best_match = problem_key
                best_solution = solution
        
        return (best_match, best_solution) if best_score > 0 else (None, None)
    
    def _generate_specific_questions(self, incident_type: str) -> str:
        """
        Generar preguntas espec√≠ficas basadas en los problemas disponibles en el archivo.
        
        Args:
            incident_type: Tipo de incidencia identificada
            
        Returns:
            Mensaje con preguntas espec√≠ficas dirigidas
        """
        if incident_type not in self.incident_types:
            return "¬øPodr√≠as describir el problema con m√°s detalle?"
        
        incident_data = self.incident_types[incident_type]
        incident_name = incident_data.get("name", incident_type)
        
        # Obtener problemas disponibles
        available_problems = []
        if "problemas" in incident_data:
            available_problems = list(incident_data["problemas"].keys())
        elif "common_issues" in incident_data:
            available_problems = incident_data["common_issues"]
        
        if not available_problems:
            return f"Necesito m√°s informaci√≥n sobre el problema con {incident_name.lower()}. ¬øPodr√≠as ser m√°s espec√≠fico?"
        
        # Crear mensaje con opciones espec√≠ficas
        message = f"Entiendo que hay un problema con {incident_name.lower()}. "
        
        if len(available_problems) <= 6:
            # Si hay pocas opciones, listarlas todas
            message += "Seg√∫n nuestro cat√°logo, los problemas m√°s comunes son:\n\n"
            for i, problem in enumerate(available_problems, 1):
                message += f"‚Ä¢ {problem}\n"
            message += f"\n¬øCu√°l de estos describe mejor tu situaci√≥n?"
        else:
            # Si hay muchas opciones, mostrar las m√°s relevantes
            message += "Para ayudarte mejor, ¬øel problema es que:\n\n"
            # Tomar las primeras 5-6 opciones m√°s comunes
            for i, problem in enumerate(available_problems[:6], 1):
                message += f"‚Ä¢ {problem}\n"
            message += f"‚Ä¢ Otro problema diferente\n"
            message += f"\n¬øCu√°l de estas opciones se acerca m√°s a tu situaci√≥n?"
        
        return message
    
    def _should_generate_specific_questions(self, decision: ClassificationDecision, attempt_number: int) -> bool:
        """
        Determinar si se deben generar preguntas espec√≠ficas basadas en el archivo.
        
        Args:
            decision: Decisi√≥n del LLM
            attempt_number: N√∫mero de intento
            
        Returns:
            True si debe generar preguntas espec√≠ficas
        """
        return (
            decision.incident_identified and 
            decision.incident_type and 
            not decision.problem_identified and
            attempt_number <= 3  # Solo en los primeros intentos
        )
    
    def _generate_targeted_questions(self, state: EroskiState, decision: ClassificationDecision, attempt_number: int) -> Command:
        """
        Generar preguntas dirigidas basadas en los problemas disponibles en el archivo.
        
        Args:
            state: Estado actual
            decision: Decisi√≥n del LLM
            attempt_number: N√∫mero de intento
            
        Returns:
            Command con preguntas espec√≠ficas
        """
        
        # Generar mensaje con preguntas espec√≠ficas
        specific_message = self._generate_specific_questions(decision.incident_type)
        
        # Actualizar datos de clasificaci√≥n
        classify_data = state.get("classify_data", {})
        classify_data.update({
            "incident_type": decision.incident_type,
            "incident_identified": True,
            "specific_questions_generated": True,
            "questions_source": "archivo_problemas"
        })
        
        return Command(
            update={
                "messages": state["messages"] + [AIMessage(content=specific_message)],
                "classify_data": classify_data,
                "classify_attempt_number": attempt_number,
                "current_step": "classify"
            }
        )
    
    def _try_auto_classify_from_file(self, state: EroskiState, decision: ClassificationDecision, attempt_number: int) -> Command:
        """
        Intentar clasificar autom√°ticamente usando el archivo cuando se conoce el tipo pero no el problema.
        
        Args:
            state: Estado actual
            decision: Decisi√≥n del LLM
            attempt_number: N√∫mero de intento
            
        Returns:
            Command con la acci√≥n a tomar
        """
        
        # Obtener historial de mensajes del usuario
        user_messages = []
        for msg in state.get("messages", []):
            if isinstance(msg, HumanMessage):
                user_messages.append(msg.content)
        
        # Combinar todos los mensajes del usuario
        combined_description = " ".join(user_messages)
        
        # Buscar coincidencia en el archivo
        problem_match, solution = self._find_best_matching_problem(
            decision.incident_type, 
            combined_description
        )
        
        if problem_match and solution:
            # ¬°Encontramos coincidencia! Proporcionar soluci√≥n
            solution_message = f"""‚úÖ **Incidencia Identificada: {decision.incident_type}**

üìã **Problema:** {problem_match}

üîß **Soluci√≥n:**
{solution}

¬øEsta soluci√≥n resolvi√≥ tu problema? Si persiste o necesitas m√°s ayuda, h√°zmelo saber."""
            
            classify_data = state.get("classify_data", {})
            classify_data.update({
                "incident_type": decision.incident_type,
                "specific_problem": problem_match,
                "proposed_solution": solution,
                "solution_source": "archivo_automatico",
                "problem_identified": True,
                "solution_ready": True,
                "completed": True
            })
            
            return Command(
                update={
                    "messages": state["messages"] + [AIMessage(content=solution_message)],
                    "classify_data": classify_data,
                    "current_step": "search_solution",
                    "classification_completed": True
                }
            )
        
        else:
            # No encontramos coincidencia autom√°tica, continuar con preguntas
            classify_data = state.get("classify_data", {})
            classify_data.update({
                "incident_type": decision.incident_type,
                "incident_identified": True,
                "auto_classify_attempted": True
            })
            
            return Command(
                update={
                    "messages": state["messages"] + [AIMessage(content=decision.message_to_user)],
                    "classify_data": classify_data,
                    "classify_attempt_number": attempt_number,
                    "current_step": "classify"
                }
            )
    
    def _get_conversation_history(self, state: EroskiState) -> str:
        """Obtener historial de conversaci√≥n formateado"""
        messages = state.get("messages", [])
        
        # Filtrar solo mensajes del usuario (HumanMessage)
        user_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                user_messages.append(f"Usuario: {msg.content}")
        
        if not user_messages:
            return "No hay mensajes previos del usuario"
        
        return "\n".join(user_messages)
    
    def _get_full_conversation_history(self, state: EroskiState) -> str:
        """Obtener historial completo incluyendo respuestas del bot"""
        messages = state.get("messages", [])
        
        formatted_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                formatted_messages.append(f"üë§ Usuario: {msg.content}")
            elif isinstance(msg, AIMessage):
                formatted_messages.append(f"ü§ñ Bot: {msg.content}")
        
        if not formatted_messages:
            return "No hay historial de conversaci√≥n"
        
        return "\n".join(formatted_messages)
    
    async def _analyze_historical_messages(self, state: EroskiState) -> Command:
        """
        ‚úÖ NUEVA FUNCIONALIDAD: Analizar mensajes hist√≥ricos en la primera visita
        
        Esta funci√≥n se ejecuta solo en el primer intento de clasificaci√≥n
        para revisar todo el historial y extraer informaci√≥n ya proporcionada.
        """
        
        self.logger.info("üîç Analizando historial completo para informaci√≥n previa...")
        
        # Preparar datos para el an√°lisis hist√≥rico
        auth_data = state.get("auth_data_collected", {})
        
        prompt_input = {
            "incident_types_info": self._format_incident_types_for_llm(),
            "conversation_history": self._get_full_conversation_history(state),
            "employee_name": auth_data.get("name", "No especificado"),
            "store_name": auth_data.get("store_name", "No especificado"),
            "section": auth_data.get("section", "No especificado")
        }
        
        # Ejecutar an√°lisis hist√≥rico con LLM
        formatted_prompt = self.historical_analysis_prompt.format(**prompt_input)
        
        self.logger.info("ü§ñ Ejecutando an√°lisis hist√≥rico inicial")
        
        try:
            response = await self.llm.ainvoke(formatted_prompt)
            
            # Parsear respuesta
            decision_data = self.parser.parse(response.content)
            decision = ClassificationDecision(**decision_data)
            
            self.logger.info(f"‚úÖ An√°lisis hist√≥rico completado: {decision.next_action}")
            self.logger.info(f"üîç Info hist√≥rica encontrada: {decision.historical_info_found}")
            
            # Actualizar datos de clasificaci√≥n con informaci√≥n hist√≥rica
            classify_data = {
                "incident_identified": decision.incident_identified,
                "problem_identified": decision.problem_identified,
                "solution_ready": decision.solution_ready,
                "confidence_level": decision.confidence_level,
                "keywords_detected": decision.keywords_detected,
                "urgency_level": decision.urgency_level,
                "historical_info_found": decision.historical_info_found,
                "last_analysis": datetime.now().isoformat()
            }
            
            # Agregar informaci√≥n espec√≠fica si se identific√≥
            if decision.incident_type:
                classify_data["incident_type"] = decision.incident_type
            if decision.specific_problem:
                classify_data["specific_problem"] = decision.specific_problem
            if decision.proposed_solution:
                classify_data["proposed_solution"] = decision.proposed_solution
            
            # Procesar seg√∫n la decisi√≥n del an√°lisis hist√≥rico
            if decision.next_action == "provide_solution" and decision.solution_ready:
                return self._provide_solution_and_complete(state, decision)
            
            elif decision.needs_escalation:
                return self._escalate_to_supervisor(state)
            
            else:
                # Continuar conversaci√≥n con informaci√≥n del an√°lisis hist√≥rico
                return Command(
                    update={
                        "messages": state["messages"] + [AIMessage(content=decision.message_to_user)],
                        "classify_data": classify_data,
                        "classify_attempt_number": 1,
                        "current_step": "classify"
                    }
                )
                
        except Exception as e:
            self.logger.error(f"‚ùå Error en an√°lisis hist√≥rico: {e}")
            
            # Fallback: continuar con an√°lisis normal
            return Command(
                update={
                    "messages": state["messages"] + [AIMessage(content="Veo que mencionaste un problema. ¬øPodr√≠as contarme m√°s detalles sobre qu√© est√° ocurriendo exactamente?")],
                    "classify_data": {"last_analysis": datetime.now().isoformat()},
                    "classify_attempt_number": 1,
                    "current_step": "classify"
                }
            )
    
    def _is_classification_complete(self, state: EroskiState) -> bool:
        """Verificar si la clasificaci√≥n est√° completa"""
        classify_data = state.get("classify_data", {})
        
        return (
            classify_data.get("incident_identified", False) and
            classify_data.get("problem_identified", False) and
            classify_data.get("solution_ready", False)
        )
    
    def _should_escalate(self, state: EroskiState, attempt_number: int) -> bool:
        """Determinar si se debe escalar a supervisor"""
        classify_data = state.get("classify_data", {})
        
        # Escalaci√≥n autom√°tica por n√∫mero de intentos
        if attempt_number >= 6:
            return True
        
        # Escalaci√≥n si el LLM lo indica
        if classify_data.get("needs_escalation", False):
            return True
        
        # Escalaci√≥n si el usuario lo solicita expl√≠citamente
        last_message = self._get_last_user_message(state)
        escalation_keywords = ["supervisor", "jefe", "responsable", "hablar con alguien mas", "no me ayudas"]
        
        if any(keyword in last_message.lower() for keyword in escalation_keywords):
            return True
        
        return False
    
    def _get_last_user_message(self, state: EroskiState) -> str:
        """Obtener el √∫ltimo mensaje del usuario"""
        messages = state.get("messages", [])
        
        for msg in reversed(messages):
            if isinstance(msg, HumanMessage):
                return msg.content
        
        return ""
    
    async def execute(self, state: EroskiState) -> Command:
        """
        Ejecutar clasificaci√≥n LLM-driven.
        
        Args:
            state: Estado actual del workflow
            
        Returns:
            Command con la siguiente acci√≥n
        """
        
        self.logger.info("üèÖ" * 50)
        self.logger.info(f"üåÑ Entrando en clasificaci√≥n LLM-driven")
        
        try:
            self.logger.info("üîç Iniciando clasificaci√≥n de incidencia")
            
            # 1. Verificar si ya est√° completo
            if self._is_classification_complete(state):
                return self._proceed_to_next_step(state)
            
            # 2. Preparar datos
            attempt_number = state.get("classify_attempt_number", 0) + 1
            
            # 3. ‚úÖ NUEVA FUNCIONALIDAD: An√°lisis inicial del historial
            if attempt_number == 1:
                self.logger.info("üîç Primera visita: analizando historial completo")
                return await self._analyze_historical_messages(state)
            
            # 4. Verificar escalaci√≥n
            if self._should_escalate(state, attempt_number):
                return self._escalate_to_supervisor(state)
            
            # 5. Verificar cancelaci√≥n
            if self._wants_to_cancel(state):
                return self._handle_cancellation(state)
            
            # 6. Ejecutar an√°lisis LLM
            decision = await self._analyze_with_llm(state, attempt_number)
            
            # 7. Procesar decisi√≥n
            return await self._process_llm_decision(state, decision, attempt_number)
            
        except Exception as e:
            self.logger.error(f"‚ùå Error en clasificaci√≥n: {e}")
            return self._handle_error(state, str(e))
    
    async def _analyze_with_llm(self, state: EroskiState, attempt_number: int) -> ClassificationDecision:
        """Analizar con LLM para tomar decisi√≥n (para intentos posteriores al primero)"""
        
        # Preparar datos para el prompt
        auth_data = state.get("auth_data_collected", {})
        classify_data = state.get("classify_data", {})
        user_message = self._get_last_user_message(state)
        
        # Crear prompt para an√°lisis continuo (no hist√≥rico)
        prompt_input = {
            "incident_types_info": self._format_incident_types_for_llm(),
            "conversation_history": self._get_conversation_history(state),
            "employee_name": auth_data.get("name", "No especificado"),
            "store_name": auth_data.get("store_name", "No especificado"),
            "section": auth_data.get("section", "No especificado"),
            "attempt_number": attempt_number,
            "previous_incident_type": classify_data.get("incident_type"),
            "previous_problem": classify_data.get("specific_problem"),
            "user_message": user_message
        }
        
        # Usar un prompt m√°s simple para intentos continuos
        simple_prompt = PromptTemplate(
            template="""Contin√∫a analizando la consulta del usuario para clasificar la incidencia.

INFORMACI√ìN PREVIA IDENTIFICADA:
- Tipo de incidencia: {previous_incident_type}
- Problema espec√≠fico: {previous_problem}
- Intento n√∫mero: {attempt_number}

TIPOS DE INCIDENCIAS DISPONIBLES:
{incident_types_info}

√öLTIMO MENSAJE DEL USUARIO:
"{user_message}"

CONTEXTO DEL EMPLEADO:
- Nombre: {employee_name}
- Secci√≥n: {section}

INSTRUCCIONES CR√çTICAS:
1. Si ya identificaste el tipo, enf√≥cate en el problema espec√≠fico
2. ‚ùå NO hagas preguntas gen√©ricas como "cu√©ntame m√°s detalles"
3. ‚úÖ Si no sabes el problema espec√≠fico, lista los problemas disponibles del archivo para ese tipo y pregunta cu√°l coincide
4. ‚úÖ Si ya tienes el problema espec√≠fico, proporciona la soluci√≥n DEL ARCHIVO
5. Si el usuario confirma o rechaza algo, act√∫a en consecuencia
6. Si es confuso despu√©s de varios intentos, considera escalaci√≥n

EJEMPLO DE BUENA PREGUNTA ESPEC√çFICA:
"Entiendo que hay un problema con la balanza. ¬øEl problema es que:
‚Ä¢ La balanza no imprime las etiquetas
‚Ä¢ La balanza no se enciende
‚Ä¢ El precio en la etiqueta es incorrecto
‚Ä¢ La pantalla no responde
¬øCu√°l de estos describe tu situaci√≥n?"

RESPONDE √öNICAMENTE CON JSON V√ÅLIDO:
{{
    "incident_identified": false,
    "problem_identified": false,
    "solution_ready": false,
    "needs_escalation": false,
    "wants_to_cancel": false,
    "incident_type": null,
    "specific_problem": null,
    "proposed_solution": null,
    "confidence_level": 0.0,
    "next_action": "ask_questions",
    "message_to_user": "Mensaje natural aqu√≠",
    "questions_to_ask": [],
    "keywords_detected": [],
    "urgency_level": 2
}}""",
            input_variables=[
                "incident_types_info", "employee_name", "section", 
                "attempt_number", "previous_incident_type", "previous_problem", "user_message"
            ]
        )
        
        # Ejecutar LLM
        formatted_prompt = simple_prompt.format(**prompt_input)
        
        self.logger.info(f"ü§ñ Ejecutando an√°lisis LLM continuo (intento {attempt_number})")
        
        response = await self.llm.ainvoke(formatted_prompt)
        
        # Parsear respuesta
        try:
            decision_data = self.parser.parse(response.content)
            decision = ClassificationDecision(**decision_data)
            
            self.logger.info(f"‚úÖ Decisi√≥n LLM: {decision.next_action}")
            return decision
            
        except Exception as e:
            self.logger.error(f"‚ùå Error parseando respuesta LLM: {e}")
            self.logger.error(f"Respuesta raw: {response.content}")
            
            # Fallback: crear decisi√≥n b√°sica
            return ClassificationDecision(
                next_action="ask_questions",
                message_to_user="Disculpa, necesito m√°s informaci√≥n. ¬øPodr√≠as describir el problema con m√°s detalle?",
                confidence_level=0.0
            )
    
    async def _process_llm_decision(self, state: EroskiState, decision: ClassificationDecision, attempt_number: int) -> Command:
        """Procesar la decisi√≥n del LLM"""
        
        # Actualizar datos de clasificaci√≥n
        classify_data = state.get("classify_data", {})
        
        # Actualizar con nueva informaci√≥n
        if decision.incident_type:
            classify_data["incident_type"] = decision.incident_type
        if decision.specific_problem:
            classify_data["specific_problem"] = decision.specific_problem
        if decision.proposed_solution:
            classify_data["proposed_solution"] = decision.proposed_solution
        
        classify_data.update({
            "incident_identified": decision.incident_identified,
            "problem_identified": decision.problem_identified,
            "solution_ready": decision.solution_ready,
            "confidence_level": decision.confidence_level,
            "keywords_detected": decision.keywords_detected,
            "urgency_level": decision.urgency_level,
            "last_analysis": datetime.now().isoformat()
        })
        
        # Procesar seg√∫n la acci√≥n
        if decision.next_action == "escalate" or decision.needs_escalation:
            return self._escalate_to_supervisor(state)
        
        elif decision.next_action == "provide_solution" and decision.solution_ready:
            return self._provide_solution_and_complete(state, decision)
        
        elif decision.next_action == "complete" and decision.solution_ready:
            return self._provide_solution_and_complete(state, decision)
        
        # ‚úÖ NUEVO: Si se identifica tipo pero no problema espec√≠fico, buscar en archivo
        elif decision.incident_identified and decision.incident_type and not decision.problem_identified:
            return self._try_auto_classify_from_file(state, decision, attempt_number)
        
        # ‚úÖ NUEVO: Si necesita m√°s informaci√≥n, generar preguntas espec√≠ficas del archivo
        elif self._should_generate_specific_questions(decision, attempt_number):
            return self._generate_targeted_questions(state, decision, attempt_number)
        
        else:
            # Continuar conversaci√≥n
            return Command(
                update={
                    "messages": state["messages"] + [AIMessage(content=decision.message_to_user)],
                    "classify_data": classify_data,
                    "classify_attempt_number": attempt_number,
                    "current_step": "classify"
                }
            )
    
    def _provide_solution_and_complete(self, state: EroskiState, decision: ClassificationDecision) -> Command:
        """Proporcionar soluci√≥n y completar clasificaci√≥n"""
        
        classify_data = state.get("classify_data", {})
        incident_type = decision.incident_type or classify_data.get("incident_type")
        
        # ‚úÖ NUEVO: Buscar soluci√≥n en el archivo JSON
        solution_from_file = None
        if incident_type and decision.specific_problem:
            solution_from_file = self._get_specific_solution(incident_type, decision.specific_problem)
        
        # Usar soluci√≥n del archivo si existe, sino la del LLM
        final_solution = solution_from_file or decision.proposed_solution
        
        if not final_solution:
            # Si no hay soluci√≥n espec√≠fica, escalamos
            return self._escalate_to_supervisor(state)
        
        # Crear mensaje completo con soluci√≥n
        solution_message = f"""‚úÖ **Incidencia Identificada: {incident_type}**

üìã **Problema:** {decision.specific_problem}

üîß **Soluci√≥n:**
{final_solution}

¬øEsta soluci√≥n resolvi√≥ tu problema? Si persiste o necesitas m√°s ayuda, h√°zmelo saber."""
        
        return Command(
            update={
                "messages": state["messages"] + [AIMessage(content=solution_message)],
                "classify_data": {
                    **classify_data, 
                    "completed": True,
                    "solution_provided": final_solution,
                    "solution_source": "archivo" if solution_from_file else "llm"
                },
                "incident_type": incident_type,
                "current_step": "search_solution",  # Pasar al siguiente nodo
                "classification_completed": True
            }
        )
    
    def _escalate_to_supervisor(self, state: EroskiState) -> Command:
        """Escalar a supervisor"""
        
        escalation_message = """üîù **Escalando a Supervisor**

No he podido identificar completamente tu incidencia o necesitas atenci√≥n especializada. 

Te estoy conectando con un supervisor que podr√° ayudarte mejor. Mientras tanto, ten preparada la siguiente informaci√≥n:
- Descripci√≥n detallada del problema
- Qu√© intentaste hacer cuando ocurri√≥
- Si hay c√≥digos de error visibles
- N√∫mero de serie del equipo (si aplica)

Un supervisor se pondr√° en contacto contigo pronto."""
        
        return Command(
            update={
                "messages": state["messages"] + [AIMessage(content=escalation_message)],
                "current_step": "escalate",
                "escalation_reason": "classification_failed",
                "escalation_data": state.get("classify_data", {})
            }
        )
    
    def _proceed_to_next_step(self, state: EroskiState) -> Command:
        """Proceder al siguiente paso del workflow"""
        
        classify_data = state.get("classify_data", {})
        
        completion_message = f"""‚úÖ **Clasificaci√≥n Completada**

He identificado tu incidencia como: **{classify_data.get('incident_type', 'N/A')}**

Ahora voy a buscar soluciones espec√≠ficas para tu problema."""
        
        return Command(
            update={
                "messages": state["messages"] + [AIMessage(content=completion_message)],
                "current_step": "search_solution",
                "incident_type": classify_data.get("incident_type"),
                "classification_completed": True
            }
        )
    
    def _wants_to_cancel(self, state: EroskiState) -> bool:
        """Verificar si el usuario quiere cancelar"""
        last_message = self._get_last_user_message(state).lower()
        cancel_keywords = ["cancelar", "salir", "terminar", "no quiero", "olv√≠dalo"]
        
        return any(keyword in last_message for keyword in cancel_keywords)
    
    def _handle_cancellation(self, state: EroskiState) -> Command:
        """Manejar cancelaci√≥n del usuario"""
        
        cancel_message = """‚ùå **Proceso Cancelado**

Entiendo que no quieres continuar con el reporte de incidencia.

Si cambias de opini√≥n o tienes otro problema, puedes volver a iniciar el proceso en cualquier momento.

¬°Que tengas un buen d√≠a!"""
        
        return Command(
            update={
                "messages": state["messages"] + [AIMessage(content=cancel_message)],
                "current_step": "finalize",
                "conversation_ended": True,
                "cancellation_reason": "user_request"
            }
        )
    
    def _handle_error(self, state: EroskiState, error_message: str) -> Command:
        """Manejar errores de manera elegante"""
        
        error_response = """‚ö†Ô∏è **Error Temporal**

Ha ocurrido un problema t√©cnico durante la clasificaci√≥n de tu incidencia.

Por favor, intenta describir tu problema de nuevo o solicita hablar con un supervisor.

Error t√©cnico registrado para nuestro equipo de sistemas."""
        
        return Command(
            update={
                "messages": state["messages"] + [AIMessage(content=error_response)],
                "current_step": "escalate",
                "error_occurred": True,
                "error_details": error_message
            }
        )


# =============================================================================
# FUNCI√ìN PARA CREAR INSTANCIA
# =============================================================================

async def llm_driven_classify_node(state: EroskiState) -> EroskiState:
    """
    Funci√≥n wrapper para LangGraph - Nodo Classify LLM-driven
    
    Args:
        state: Estado actual como dict
        
    Returns:
        Estado actualizado como dict
    """
    
    # Crear instancia del nodo
    node = LLMDrivenClassifyNode()
    
    # Ejecutar el nodo (retorna Command)
    command = await node.execute(state)
    
    # Aplicar las actualizaciones al estado
    updated_state = {**state, **command.update}
    
    # Logging para verificar actualizaciones
    logging.getLogger("Node.classify").info("üîç === CLASIFICACI√ìN: ACTUALIZACIONES APLICADAS ===")
    for key, value in command.update.items():
        logging.getLogger("Node.classify").info(f"üîß {key}: {value}")
    logging.getLogger("Node.classify").info("üîç === FIN ACTUALIZACIONES CLASIFICACI√ìN ===")
    
    return updated_state


__all__ = ["llm_driven_classify_node", "LLMDrivenClassifyNode"]